{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions that are not covered : 0 out of  13086\n",
      "Number of tags in sample : 838\n",
      "Number of data points in train data : (10468, 838)\n",
      "Number of data points in test data : (2618, 838)\n",
      "Dimensions of train data X: (10468, 96717) Y : (10468, 838)\n",
      "Dimensions of test data X: (2618, 96717) Y: (2618, 838)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re, sys, os, json\n",
    "import datetime as dt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIRECTORY = \"../Results\"\n",
    "RES_DIR = \"../Results\"\n",
    "fpath = f\"{DATA_DIRECTORY}/history.stackexchange.com/Posts/posts.json\"\n",
    "\n",
    "with open(fpath, \"r\", encoding=\"utf8\") as datajs:\n",
    "    data_arr = json.load(datajs)[\"Posts\"]\n",
    "\n",
    "df = pd.DataFrame(columns=[\"title\", \"tags\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def striphtml(data):\n",
    "    cleanr = re.compile(\"<.*?>\")\n",
    "    cleantext = re.sub(cleanr, \" \", str(data))\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "for id, post in data_arr.items():\n",
    "    title = post[\"title\"]\n",
    "    title = striphtml(title.encode(\"utf-8\"))\n",
    "\n",
    "    tags = post[\"tags\"]\n",
    "    tags = re.sub(r\"[<>]\", \" \", tags)\n",
    "    tags = \" \".join(tags.split())\n",
    "    title = re.sub(r\"[^a-zA-Z]+\", \" \", title)\n",
    "\n",
    "    words = word_tokenize(str(title.lower()))\n",
    "    title = \" \".join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j) != 1))\n",
    "    df.loc[len(df)] = [title, tags]\n",
    "\n",
    "df = df[[\"title\", \"tags\"]]\n",
    "df[\"title\"] = df[\"title\"].astype(\"str\")\n",
    "# print(df)\n",
    "\n",
    "y_vectorizer = CountVectorizer(lambda x: x.split(), binary=True)\n",
    "multilabel_output = y_vectorizer.fit_transform(df[\"tags\"])\n",
    "\n",
    "\n",
    "def tags_to_choose(n):\n",
    "    \"\"\"\n",
    "    Choose first n tags only.\n",
    "    \"\"\"\n",
    "\n",
    "    t = multilabel_output.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_outputn = multilabel_output[:, sorted_tags_i[:n]]\n",
    "    return multilabel_outputn\n",
    "\n",
    "\n",
    "def questions_explained_fn(n):\n",
    "    multilabel_outputn = tags_to_choose(n)\n",
    "    x = multilabel_outputn.sum(axis=1)\n",
    "    return np.count_nonzero(x == 0)\n",
    "\n",
    "\n",
    "question_explained = []\n",
    "total_tags = multilabel_output.shape[1]\n",
    "total_qs = df.shape[0]\n",
    "\n",
    "for i in range(500, total_tags, 100):\n",
    "    question_explained.append(np.round(((total_qs - questions_explained_fn(i)) / total_qs) * 100, 3))\n",
    "\n",
    "multilabel_yx = tags_to_choose(5500)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(5500), \"out of \", total_qs)\n",
    "\n",
    "multilabel_yx.get_shape()\n",
    "\n",
    "print(\"Number of tags in sample :\", multilabel_output.shape[1])\n",
    "\n",
    "total_size = df.shape[0]\n",
    "train_size = int(0.80 * total_size)\n",
    "\n",
    "x_train = df.head(train_size)\n",
    "x_test = df.tail(total_size - train_size)\n",
    "\n",
    "y_train = multilabel_yx[0:train_size, :]\n",
    "y_test = multilabel_yx[train_size:total_size, :]\n",
    "\n",
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    min_df=0.00009,\n",
    "    max_features=200000,\n",
    "    smooth_idf=True,\n",
    "    norm=\"l2\",\n",
    "    tokenizer=lambda x: x.split(),\n",
    "    sublinear_tf=False,\n",
    "    ngram_range=(1, 3),\n",
    ")\n",
    "\n",
    "x_train_vectors = tfidf_vect.fit_transform(x_train[\"title\"])\n",
    "x_test_vectors = tfidf_vect.transform(x_test[\"title\"])\n",
    "\n",
    "print(\"Dimensions of train data X:\", x_train_vectors.shape, \"Y :\", y_train.shape)\n",
    "print(\"Dimensions of test data X:\", x_test_vectors.shape, \"Y:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SGDClassifier(alpha=1e-05, loss='log',\n",
       "                                            penalty='l1'),\n",
       "                    n_jobs=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = OneVsRestClassifier(SGDClassifier(loss=\"log\", alpha=0.00001, penalty=\"l1\"), n_jobs=2)\n",
    "\n",
    "classifier.fit(x_train_vectors.toarray(), y_train.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  0.04583651642475172\n",
      "macro f1 score  0.18258544432149323\n",
      "micro f1 score  0.4461939407101748\n",
      "hamming loss  0.004649288658835198\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(x_test_vectors)\n",
    "\n",
    "print(\"accuracy \", metrics.accuracy_score(y_test, predictions))\n",
    "print(\"macro f1 score \", metrics.f1_score(y_test, predictions, average=\"macro\"))\n",
    "print(\"micro f1 score \", metrics.f1_score(y_test, predictions, average=\"micro\"))\n",
    "print(\"hamming loss \", metrics.hamming_loss(y_test, predictions))\n",
    "\n",
    "import joblib\n",
    "_ = joblib.dump(classifier, \"tag_pred.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = joblib.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
